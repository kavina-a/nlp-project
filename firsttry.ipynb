{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1135e2-4877-4f9c-8436-ad91a5a94871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import os\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mray\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray'"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4830c-20bf-421c-947d-06cf1fbb94b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/Users/kavina/Documents/EXTRA/madewithml/Made-With-ML\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727d50b-d391-4a5f-a839-60328f76346f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1e4ff-b699-479b-b925-44100a103d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d28cf-454c-4e00-beb3-5bedabd87e02",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f314cec-e498-4c9d-9d5b-f651a099ef63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce4425-f7bc-41a5-8701-d54fa70eac30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(DATASET_LOC)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbadda48-8c4b-47be-bcf7-5ebe96652461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the dataset into train and val\n",
    "test_size = 0.2\n",
    "train_df, val_df = train_test_split(df, stratify=df.tag, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51e1bd-e4f6-4aa3-a431-34d6d84b15b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_df.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ddba0a-05c5-41af-81d9-74f363b00d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_df.tag.value_counts() * int((1-test_size) / test_size)\n",
    "# validating if value counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1463c9c4-88b8-4766-9a60-6f403c2d379d",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c485e39-d14e-4024-bcde-8d3e7d9713a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to understand the signals and nuances of our dataset\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud, STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef8eaa-0515-4576-980d-c0d31e5deac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the most common tags \n",
    "all_tags = Counter(df.tag)\n",
    "all_tags.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69047d45-c866-4479-b8c8-99efad8e2b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot tag frequencies - data visualization\n",
    "tags, tag_counts = zip(*all_tags.most_common())\n",
    "plt.figure(figsize=(10, 3))\n",
    "ax = sns.barplot(x=list(tags), y=list(tag_counts))\n",
    "ax.set_xticklabels(tags, rotation=0, fontsize=8)\n",
    "plt.title(\"Tag distribution\", fontsize=14)\n",
    "plt.ylabel(\"# of projects\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c45e72-1aad-459d-9dfb-8015ad8309f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Most frequent tokens for each tag using word cloud \n",
    "tag=\"natural-language-processing\"\n",
    "# tag=\"computer-vision\"\n",
    "plt.figure(figsize=(10, 3))\n",
    "subset = df[df.tag==tag]\n",
    "text = subset.title.values\n",
    "cloud = WordCloud(\n",
    "    stopwords=STOPWORDS, background_color=\"black\", collocations=False,\n",
    "    width=500, height=300).generate(\" \".join(text))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3580c82-b8dc-4f43-b25a-4538bb423a78",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc763d-a338-4bfd-9c3f-4d748bfa5d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preparation\n",
    "import numpy as np\n",
    "\n",
    "# View rows at index 4 and 10\n",
    "print(df.loc[[4, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c3911-22c0-4ab5-aa81-4940ae791be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46242923-e1b3-403c-9ed6-ad378957a3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"text\"] = df.title + \" \" + df.description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008cd873-dc54-40d1-8430-d9218c62c1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = stopwords.words(\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4f2e0-e62d-4749-8244-e69766c5ae06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text, stopwords=STOPWORDS):\n",
    "    text = text.lower() # turn into lower case\n",
    "    \n",
    "    pattern = re.compile(r'\\b(' + r\"|\".join(stopwords) + r\")\\b\\s*\") # remove stopwords\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)  # add spacing\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n",
    "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
    "    text = text.strip()  # strip white space at the ends\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  #  remove links\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50775a88-48e4-472a-ba39-c9905ccb0662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_df = df.copy()\n",
    "df.text = df.text.apply(clean_text)\n",
    "print (f\"{original_df.text.values[0]}\\n{df.text.values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8201db-1ac6-44a9-83b6-d52f29795e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"id\", \"created_on\", \"title\", \"description\"], errors=\"ignore\") \n",
    "df = df.dropna(subset=[\"tag\"])  # drop nulls\n",
    "df = df[[\"text\", \"tag\"]] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9883ca-fc54-4346-9398-2bfba89befa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22cac3-76a1-4511-ab1f-29843a68ea00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encoding tags \n",
    "\n",
    "tags = train_df.tag.unique().tolist()\n",
    "num_classes = len(tags)\n",
    "class_to_index = {tag: i for i, tag in enumerate(tags)}\n",
    "class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e93664-4efd-476b-88cd-c50ed05cb3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556746d8-0937-4e4d-99d0-b257bba697dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"tag\"] = df[\"tag\"].map(class_to_index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93d2bd-5110-414c-b8f0-4cb63a54e3d4",
   "metadata": {},
   "source": [
    "## Encoding using a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b2a49-5cce-4361-942e-9287cb105b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using scibert\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50237d9-feaf-4bbc-b3e8-3e3dfa654031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bert tokenizer - testing \n",
    "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "text = \"Transfer learning with transformers for text classification.\"\n",
    "encoded_inputs = tokenizer([text], return_tensors=\"np\", padding=\"longest\")  \n",
    "print (\"input_ids:\", encoded_inputs[\"input_ids\"])\n",
    "print (\"attention_mask:\", encoded_inputs[\"attention_mask\"])\n",
    "print (tokenizer.decode(encoded_inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ccff77-e51b-4e0d-b2b7-318a57b586c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "    encoded_inputs = tokenizer(batch[\"text\"].tolist(), return_tensors=\"np\", padding=\"longest\")\n",
    "    return dict(ids=encoded_inputs[\"input_ids\"], masks=encoded_inputs[\"attention_mask\"], targets=np.array(batch[\"tag\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d09eb1-9525-4c8c-a4d2-6229c674099e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenize(df.head(1)) #test tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd640a57-3ff7-4ed1-8f6a-f0f50b4bf45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#combining the whole preprocessing technique \n",
    "def preprocess(df, class_to_index):\n",
    "    \"\"\"Preprocess the data.\"\"\"\n",
    "    df[\"text\"] = df.title + \" \" + df.description  # feature engineering\n",
    "    df[\"text\"] = df.text.apply(clean_text)  # clean text\n",
    "    df = df.drop(columns=[\"id\", \"created_on\", \"title\", \"description\"], errors=\"ignore\")  # clean dataframe\n",
    "    df = df[[\"text\", \"tag\"]]  # rearrange columns\n",
    "    df[\"tag\"] = df[\"tag\"].map(class_to_index)  # label encoding\n",
    "    outputs = tokenize(df)\n",
    "    return outputs\n",
    "\n",
    "preprocess(df=train_df, class_to_index = class_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a82db-d55f-447b-adaa-8655ea06878b",
   "metadata": {},
   "source": [
    "## Distributed PreProcessing <- Learning Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f927c90-1698-402f-b5c6-145f3abd6a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from madewithml.data import stratify_split\n",
    "\n",
    "# Please preserve the original order of the dataset when returning results, even if it means slightly slower performance\n",
    "ray.data.DatasetContext.get_current().execution_options.preserve_order = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c7342-e747-4680-961e-72f591bd7b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import pyarrow\n",
    "print(ray.__version__)\n",
    "print(pyarrow.__version__)\n",
    "ds = ray.data.read_csv(DATASET_LOC)\n",
    "ds = ds.random_shuffle(seed=42)\n",
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37657acf-8f86-437f-bb75-eb7219074ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "test_size = 0.2\n",
    "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565196ad-85f3-463d-87fb-9b991b24bd6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping\n",
    "tags = train_ds.unique(column=\"tag\")\n",
    "class_to_index = {tag: i for i, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984a3b2-fdce-452f-93ec-f93433b2a4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distributed preprocessing\n",
    "sample_ds = train_ds.map_batches(\n",
    "  preprocess,\n",
    "  fn_kwargs={\"class_to_index\": class_to_index},\n",
    "  batch_format=\"pandas\")\n",
    "sample_ds.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009b507-cd7c-4453-b695-1ec0ea85ac57",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f7910-678e-4ea2-b6c7-2852ea388331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ray.data.preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc7b8d-39e7-422f-8e82-a0e0cceeb8d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    eval(\"setattr(torch.backends.cudnn, 'deterministic', True)\")\n",
    "    eval(\"setattr(torch.backends.cudnn, 'benchmark', False)\")\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e815395-221c-4ee5-acf9-72c44c6c3b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(num_samples=None):\n",
    "    ds = ray.data.read_csv(DATASET_LOC)\n",
    "    ds = ds.random_shuffle(seed=42)\n",
    "    ds = ray.data.from_items(ds.take(num_samples)) if num_samples else ds\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e44325-dace-4af1-8f23-a3e27e059b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomPreprocessor(Preprocessor):\n",
    "    \"\"\"Custom preprocessor class.\"\"\"\n",
    "    def _fit(self, ds):\n",
    "        tags = ds.unique(column=\"tag\")\n",
    "        self.class_to_index = {tag: i for i, tag in enumerate(tags)}\n",
    "        self.index_to_class = {v:k for k, v in self.class_to_index.items()}\n",
    "    def _transform_pandas(self, batch):  # could also do _transform_numpy\n",
    "        return preprocess(batch, class_to_index=self.class_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfc701-efc4-4b87-893a-89d56af67efe",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f4104-0e56-42d3-8c42-578bd78e54ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db37c9-832a-4712-83e5-d8941131dbdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603428b6-a6fa-4d79-8d66-5edcdb0fd0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = llm.config.hidden_size\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451caa6-4a53-4e80-b036-680fa785485d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FinetunedLLM(nn.Module):\n",
    "#     def __init__(self, llm, dropout_p, embedding_dim, num_classes):\n",
    "#         super(FinetunedLLM, self).__init__()\n",
    "#         self.llm = llm\n",
    "#         self.dropout = torch.nn.Dropout(dropout_p)\n",
    "#         self.fc1 = torch.nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "#     def forward(self, batch):\n",
    "#         ids, masks = batch[\"ids\"], batch[\"masks\"]\n",
    "#         seq, pool = self.llm(input_ids=ids, attention_mask=masks)\n",
    "#         z = self.dropout(pool)\n",
    "#         z = self.fc1(z)\n",
    "#         return z\n",
    "\n",
    "#     @torch.inference_mode()\n",
    "#     def predict(self, batch):\n",
    "#         self.eval()\n",
    "#         z = self(inputs)\n",
    "#         y_pred = torch.argmax(z, dim=1).cpu().numpy()\n",
    "#         return y_pred\n",
    "\n",
    "#     @torch.inference_mode()\n",
    "#     def predict_proba(self, batch):\n",
    "#         self.eval()\n",
    "#         z = self(batch)\n",
    "#         y_probs = F.softmax(z).cpu().numpy()\n",
    "#         return y_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24181d0-eed2-46ed-b303-0ec5236487c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "from madewithml.models import FinetunedLLM\n",
    "model = FinetunedLLM(llm=llm, dropout_p=0.5, embedding_dim=embedding_dim, num_classes=num_classes)\n",
    "print (model.named_parameters) #this basically explains whats happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cce98b-3e0a-4ba4-81cf-803d7a55f102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.train.torch import get_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbf894-8f4b-4fd1-8e87-3365f1fc4f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_array(arr, dtype=np.int32):\n",
    "    max_len = max(len(row) for row in arr)\n",
    "    padded_arr = np.zeros((arr.shape[0], max_len), dtype=dtype)\n",
    "    for i, row in enumerate(arr):\n",
    "        padded_arr[i][:len(row)] = row\n",
    "    return padded_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc002f2-cb19-4ce7-94fe-66c1777a161e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch[\"ids\"] = pad_array(batch[\"ids\"])\n",
    "    batch[\"masks\"] = pad_array(batch[\"masks\"])\n",
    "    dtypes = {\"ids\": torch.int32, \"masks\": torch.int32, \"targets\": torch.int64}\n",
    "    tensor_batch = {}\n",
    "    for key, array in batch.items():\n",
    "        tensor_batch[key] = torch.as_tensor(array, dtype=dtypes[key], device=get_device())\n",
    "    return tensor_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288cfc7-b0b7-49b6-922f-e1e3f2311167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint, session\n",
    "from ray.air.config import CheckpointConfig, DatasetConfig, RunConfig, ScalingConfig\n",
    "import ray.train as train\n",
    "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_workers = 1\n",
    "resources_per_worker={\"CPU\": 1, \"GPU\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142b13b-fa78-4343-aa01-9c4c74f7b51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(ds, batch_size, model, num_classes, loss_fn, optimizer):\n",
    "    \"\"\"Train step.\"\"\"\n",
    "    model.train()\n",
    "    loss = 0.0\n",
    "    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
    "    for i, batch in enumerate(ds_generator):\n",
    "        optimizer.zero_grad()  # reset gradients\n",
    "        z = model(batch)  # forward pass\n",
    "        targets = F.one_hot(batch[\"targets\"], num_classes=num_classes).float()  # one-hot (for loss_fn)\n",
    "        J = loss_fn(z, targets)  # define loss\n",
    "        J.backward()  # backward pass\n",
    "        optimizer.step()  # update weights\n",
    "        loss += (J.detach().item() - loss) / (i + 1)  # cumulative loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416b9fd-6eb9-4780-a5fc-74856013be01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_step(ds, batch_size, model, num_classes, loss_fn):\n",
    "    \"\"\"Eval step.\"\"\"\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    y_trues, y_preds = [], []\n",
    "    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
    "    with torch.inference_mode():\n",
    "        for i, batch in enumerate(ds_generator):\n",
    "            z = model(batch)\n",
    "            targets = F.one_hot(batch[\"targets\"], num_classes=num_classes).float()  # one-hot (for loss_fn)\n",
    "            J = loss_fn(z, targets).item()\n",
    "            loss += (J - loss) / (i + 1)\n",
    "            y_trues.extend(batch[\"targets\"].cpu().numpy())\n",
    "            y_preds.extend(torch.argmax(z, dim=1).cpu().numpy())\n",
    "    return loss, np.vstack(y_trues), np.vstack(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20af9c-b8c1-4303-92c5-cc93e3e6103a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop_per_worker(config):\n",
    "    # Hyperparameters\n",
    "    dropout_p = config[\"dropout_p\"]\n",
    "    lr = config[\"lr\"]\n",
    "    lr_factor = config[\"lr_factor\"]\n",
    "    lr_patience = config[\"lr_patience\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_classes = config[\"num_classes\"]\n",
    "\n",
    "    # Get datasets\n",
    "    set_seeds()\n",
    "    train_ds = session.get_dataset_shard(\"train\")\n",
    "    val_ds = session.get_dataset_shard(\"val\")\n",
    "\n",
    "    # Model\n",
    "    llm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "    model = FinetunedLLM(llm=llm, dropout_p=dropout_p, embedding_dim=llm.config.hidden_size, num_classes=num_classes)\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    # Training components\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=lr_factor, patience=lr_patience)\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    num_workers = train.get_context().get_world_size()\n",
    "    batch_size_per_worker = batch_size // num_workers\n",
    "    for epoch in range(num_epochs):\n",
    "        # Step\n",
    "        train_loss = train_step(train_ds, batch_size_per_worker, model, num_classes, loss_fn, optimizer)\n",
    "        val_loss, _, _ = eval_step(val_ds, batch_size_per_worker, model, num_classes, loss_fn)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Checkpoint\n",
    "        metrics = dict(epoch=epoch, lr=optimizer.param_groups[0][\"lr\"], train_loss=train_loss, val_loss=val_loss)\n",
    "        checkpoint = TorchCheckpoint.from_model(model=model)\n",
    "        session.report(metrics, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5a5d7-052b-4466-b5f7-7f634722b8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train loop config\n",
    "train_loop_config = {\n",
    "    \"dropout_p\": 0.5,\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_factor\": 0.8,\n",
    "    \"lr_patience\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_classes\": num_classes,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98d78f-53c4-4200-b446-e169ef37fead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scaling config\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=num_workers,\n",
    "    use_gpu=bool(resources_per_worker[\"GPU\"]),\n",
    "    resources_per_worker=resources_per_worker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a02a977-9358-4326-9da5-4846e452f67c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run config\n",
    "checkpoint_config = CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"val_loss\", checkpoint_score_order=\"min\")\n",
    "run_config = RunConfig(name=\"llm\", checkpoint_config=checkpoint_config, local_dir=\"~/ray_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94df157-abd4-41b1-a91d-14e879cc0b2e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ecc83-a6dd-4289-bd73-bd83d2ce39e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and split the data\n",
    "ds = load_data()\n",
    "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e9465-2f2d-4b88-83db-31cda0c13546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "preprocessor = CustomPreprocessor()\n",
    "train_ds =  preprocessor.fit_transform(train_ds)\n",
    "val_ds = preprocessor.transform(val_ds)\n",
    "train_ds = train_ds.materialize()\n",
    "val_ds = val_ds.materialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd2356-6b26-4540-a0af-21331881d9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset config\n",
    "dataset_config = {\n",
    "    \"train\": DatasetConfig(fit=False, transform=False, randomize_block_order=False),\n",
    "    \"val\": DatasetConfig(fit=False, transform=False, randomize_block_order=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d8f32-f36a-4cc1-8b84-c2c34792dfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    dataset_config=dataset_config,\n",
    "    metadata={\"class_to_index\": preprocessor.class_to_index}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c8f32-b749-4808-a8f3-a85d64c35b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e9484-7e16-4a74-84e2-b63fe7bd67c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2481877-cd18-43cf-b2f9-67b37a57c7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabf0df-4304-4871-9faf-9a2d40164a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a6fc0-eb76-44d5-b15d-4c3c8be7b915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1adbc6-b3ea-4167-a6f0-812698ab39fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235afaf1-2aa2-42e0-9c21-e4a65387bfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856ae78-0ab8-40f7-b197-3993fd7a2c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7c532-9289-4679-8ca9-670db511270d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d1823-ae59-4545-bb90-78dc6d2b481e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
